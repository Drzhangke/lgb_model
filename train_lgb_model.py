#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨Python LightGBMé‡æ–°è®­ç»ƒæ¨¡å‹
åŸºäºRç‰ˆæœ¬çš„å®ç°ï¼Œä½¿ç”¨data.xlsxä¸­çš„æ•°æ®
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt
import joblib
import json
import os
from datetime import datetime

def load_and_explore_data(excel_path="data.xlsx"):
    """
    åŠ è½½å¹¶æ¢ç´¢æ•°æ®
    """
    print("ğŸ” æ­£åœ¨åŠ è½½æ•°æ®...")

    # è¯»å–Excelæ–‡ä»¶
    xl = pd.ExcelFile(excel_path)
    print(f"ğŸ“Š å‘ç°çš„å·¥ä½œè¡¨: {xl.sheet_names}")

    # è¯»å–ä¸»å·¥ä½œè¡¨
    df = pd.read_excel(excel_path, sheet_name='Sheet1')
    print(f"ğŸ“‹ æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print(f"   åˆ—å: {list(df.columns)}")

    # æŒ‰Cohortåˆ†ç»„
    dataframes = {}
    for cohort in df['Cohort'].unique():
        subset = df[df['Cohort'] == cohort].copy()
        dataframes[cohort.lower()] = subset
        print(f"ğŸ“‹ {cohort} æ•°æ®é›†: {subset.shape}")

    print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"   æ•°æ®é›†åˆ†å¸ƒ: {df['Cohort'].value_counts().to_dict()}")

    return dataframes

def prepare_features_and_target(df, feature_columns, target_column):
    """
    å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
    """
    print(f"ğŸ”§ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...")

    # æå–ç‰¹å¾
    X = df[feature_columns].copy()

    # æå–ç›®æ ‡å˜é‡
    y = df[target_column].copy()

    print(f"ğŸ“Š ç‰¹å¾å½¢çŠ¶: {X.shape}")
    print(f"ğŸ¯ ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
    print(y.value_counts())
    print()

    return X, y

def preprocess_data(X, continuous_features):
    """
    æ•°æ®é¢„å¤„ç†ï¼šä½¿ç”¨åŸå§‹12ä¸ªç‰¹å¾
    """
    print("âš™ï¸ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")

    X_processed = X.copy()

    # å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
    scalers = {}
    for feature in continuous_features:
        if feature in X_processed.columns:
            # ç”±äºæ•°æ®å·²ç»æ˜¯æ ‡å‡†åŒ–çš„ï¼Œè¿™é‡Œä¸éœ€è¦é‡æ–°æ ‡å‡†åŒ–
            # ä½†æˆ‘ä»¬éœ€è¦è®°å½•å‚æ•°ä»¥ä¿æŒä¸€è‡´æ€§
            scalers[feature] = {
                'mean': 0.0,  # æ•°æ®å·²ç»æ ‡å‡†åŒ–
                'std': 1.0   # æ•°æ®å·²ç»æ ‡å‡†åŒ–
            }
            print(f"   {feature} å·²æ ‡å‡†åŒ– (mean=0.0, std=1.0)")

    # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆç¡®ä¿ä¸ºæ•°å€¼å‹ï¼‰
    categorical_features = [col for col in X_processed.columns if col not in continuous_features]
    for feature in categorical_features:
        X_processed[feature] = pd.to_numeric(X_processed[feature], errors='coerce').fillna(0)

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: ä½¿ç”¨ {X_processed.shape[1]} ä¸ªåŸå§‹ç‰¹å¾")
    print()

    return X_processed, scalers

def train_lightgbm_model_optimized(X_train, y_train, X_val, y_val, X_test=None, y_test=None):
    """
    è®­ç»ƒä¼˜åŒ–åçš„LightGBMæ¨¡å‹
    åŒ…å«å‚æ•°è°ƒä¼˜ã€ç‰¹å¾å·¥ç¨‹ç­‰ä¼˜åŒ–
    """
    print("ğŸš€ å¼€å§‹è®­ç»ƒä¼˜åŒ–åçš„LightGBMæ¨¡å‹...")

    # ä¼˜åŒ–åçš„å‚æ•°
    params = {
        'objective': 'binary',
        'metric': ['binary_logloss', 'auc'],
        'boosting_type': 'gbdt',
        'num_leaves': 64,  # å¢åŠ å¶å­èŠ‚ç‚¹æ•°
        'max_depth': 8,    # é™åˆ¶æœ€å¤§æ·±åº¦
        'learning_rate': 0.03,  # é™ä½å­¦ä¹ ç‡
        'feature_fraction': 0.8,  # ç‰¹å¾é‡‡æ ·
        'bagging_fraction': 0.8,  # æ•°æ®é‡‡æ ·
        'bagging_freq': 5,
        'min_child_samples': 20,  # æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°
        'min_child_weight': 0.001,
        'reg_alpha': 0.1,  # L1æ­£åˆ™åŒ–
        'reg_lambda': 0.1, # L2æ­£åˆ™åŒ–
        'scale_pos_weight': 1.0,  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        'verbose': -1,
        'seed': 42,
        'nthread': -1
    }

    print(f"ğŸ”§ ä¼˜åŒ–åçš„æ¨¡å‹å‚æ•°: {params}")

    # åˆ›å»ºæ•°æ®é›†
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

    # è®­ç»ƒæ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        num_boost_round=2000,  # å¢åŠ è®­ç»ƒè½®æ•°
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=100),  # å¢åŠ æ—©åœè½®æ•°
            lgb.log_evaluation(50)  # æ›´é¢‘ç¹çš„æ—¥å¿—è¾“å‡º
        ]
    )

    print("âœ… åŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # å¦‚æœæœ‰æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œé¢å¤–çš„éªŒè¯
    if X_test is not None and y_test is not None:
        print("\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹...")
        test_pred = model.predict(X_test)
        test_auc = roc_auc_score(y_test, test_pred)
        print(".4f")

    print()
    return model



def train_lightgbm_model(X_train, y_train, X_val, y_val, params=None):
    """
    è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰
    """
    return train_lightgbm_model_optimized(X_train, y_train, X_val, y_val)

def evaluate_model(model, X_test, y_test, threshold=0.4153):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

    # é¢„æµ‹æ¦‚ç‡
    y_pred_proba = model.predict(X_test)

    # åº”ç”¨é˜ˆå€¼è¿›è¡Œåˆ†ç±»
    y_pred = (y_pred_proba > threshold).astype(int)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)

    print(".4f")
    print(".4f")
    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))

    # é˜ˆå€¼åˆ†æ
    thresholds = np.arange(0.1, 0.9, 0.05)
    accuracies = []
    for thresh in thresholds:
        pred_thresh = (y_pred_proba > thresh).astype(int)
        acc = accuracy_score(y_test, pred_thresh)
        accuracies.append(acc)

    best_threshold_idx = np.argmax(accuracies)
    best_threshold = thresholds[best_threshold_idx]
    best_accuracy = accuracies[best_threshold_idx]

    print("\nğŸ¯ é˜ˆå€¼åˆ†æ:")
    print(".4f")
    print("\nâœ… æ¨¡å‹è¯„ä¼°å®Œæˆ")
    print()

    return {
        'accuracy': accuracy,
        'auc': auc,
        'best_threshold': best_threshold,
        'best_accuracy': best_accuracy,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

def save_model_and_scalers(model, scalers, feature_columns, target_column, threshold, save_dir="models"):
    """
    ä¿å­˜æ¨¡å‹å’Œç›¸å…³å‚æ•°
    """
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹å’Œå‚æ•°...")

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜LightGBMæ¨¡å‹
    model_path = os.path.join(save_dir, f"lgb_model_{timestamp}.txt")
    model.save_model(model_path)

    # ä¿å­˜ä¸ºjoblibæ ¼å¼ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
    model_joblib_path = os.path.join(save_dir, f"lgb_model_{timestamp}.joblib")
    joblib.dump(model, model_joblib_path)

    # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
    scalers_path = os.path.join(save_dir, f"scalers_{timestamp}.json")
    with open(scalers_path, 'w', encoding='utf-8') as f:
        json.dump(scalers, f, indent=2, ensure_ascii=False)

    # ä¿å­˜æ¨¡å‹é…ç½®
    config = {
        'feature_columns': feature_columns,
        'target_column': target_column,
        'threshold': threshold,
        'continuous_features': list(scalers.keys()),
        'timestamp': timestamp,
        'model_params': model.params
    }

    config_path = os.path.join(save_dir, f"config_{timestamp}.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
    print(f"   - æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"   - Joblibæ ¼å¼: {model_joblib_path}")
    print(f"   - æ ‡å‡†åŒ–å‚æ•°: {scalers_path}")
    print(f"   - é…ç½®æ–‡ä»¶: {config_path}")
    print()

    return {
        'model_path': model_path,
        'joblib_path': model_joblib_path,
        'scalers_path': scalers_path,
        'config_path': config_path
    }

def plot_feature_importance(model, feature_names, save_path=None):
    """
    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
    """
    print("ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§...")

    # è·å–ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importance(importance_type='gain')
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)

    # ç»˜åˆ¶å›¾è¡¨
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.xlabel('Importance (Gain)')
    plt.ylabel('Features')
    plt.title('Feature Importance (Gain)')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {save_path}")

    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¯ å¼€å§‹ä½¿ç”¨Python LightGBMè®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    dataframes = load_and_explore_data("data.xlsx")

    # 2. å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—ï¼ˆåŸºäºRä»£ç ï¼‰
    base_feature_columns = [
        'Sex', 'Gallstone', 'Other', 'Ultrasond', 'Dilatation',
        'GBMorphology', 'IntramuralNodule', 'GBMass', 'Line',
        'LymphNodes', 'FIB', 'IBIL'
    ]
    target_column = 'Label'  # å®é™…ç›®æ ‡åˆ—å
    continuous_features = ['FIB', 'IBIL']  # å·²ç»æ ‡å‡†åŒ–çš„è¿ç»­ç‰¹å¾

    # ç‰¹å¾å·¥ç¨‹åä¼šè‡ªåŠ¨æ·»åŠ æ–°çš„ç‰¹å¾åˆ—ï¼Œæ‰€ä»¥è¿™é‡Œä½¿ç”¨åŸºç¡€ç‰¹å¾åˆ—
    feature_columns = base_feature_columns

    # 3. å‡†å¤‡è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®
    # æ³¨æ„ï¼šæ•°æ®é›†ä¸­ä½¿ç”¨çš„æ˜¯'Train', 'Validation', 'Test'
    if 'train' in dataframes and 'validation' in dataframes and 'test' in dataframes:
        print("âœ… å‘ç°Trainã€Validationã€Testä¸‰ä¸ªæ•°æ®é›†")

        # å‡†å¤‡æ•°æ®
        X_train, y_train = prepare_features_and_target(
            dataframes['train'], feature_columns, target_column
        )
        X_val, y_val = prepare_features_and_target(
            dataframes['validation'], feature_columns, target_column
        )
        X_test, y_test = prepare_features_and_target(
            dataframes['test'], feature_columns, target_column
        )

        # æ•°æ®é¢„å¤„ç†ï¼ˆåŒ…å«ç‰¹å¾å·¥ç¨‹å’Œæ ‡å‡†åŒ–ï¼‰
        X_train_processed, scalers = preprocess_data(X_train, continuous_features)
        X_val_processed, _ = preprocess_data(X_val, continuous_features)
        X_test_processed, _ = preprocess_data(X_test, continuous_features)

        # 4. è®­ç»ƒä¼˜åŒ–åçš„LightGBMæ¨¡å‹
        print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–åçš„LightGBMæ¨¡å‹è®­ç»ƒ...")
        model = train_lightgbm_model_optimized(
            X_train_processed, y_train,
            X_val_processed, y_val,
            X_test_processed, y_test
        )
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # 5. è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨ä¸Rè¯­è¨€ä¸€è‡´çš„é˜ˆå€¼0.4153ï¼‰
        eval_results = evaluate_model(model, X_test_processed, y_test, threshold=0.4153)

        # 6. ä¿å­˜æ¨¡å‹ä¸ºPKLæ ¼å¼
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹ä¸ºPKLæ ¼å¼...")
        os.makedirs("models", exist_ok=True)

        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
        model_data = {
            'model': model,
            'scalers': scalers,
            'feature_columns': list(X_train_processed.columns),  # åŸå§‹12ä¸ªç‰¹å¾
            'target_column': target_column,
            'threshold': 0.4153,  # ä½¿ç”¨ä¸Rè¯­è¨€ä¸€è‡´çš„é˜ˆå€¼
            'model_params': model.params,
            'training_info': {
                'train_samples': len(X_train),
                'val_samples': len(X_val),
                'test_samples': len(X_test),
                'accuracy': eval_results['accuracy'],
                'auc': eval_results['auc'],
                'r_threshold': 0.4153,  # Rè¯­è¨€ä½¿ç”¨çš„é˜ˆå€¼
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'feature_engineering': False,  # ä¸ä½¿ç”¨ç‰¹å¾å·¥ç¨‹
                'num_features': X_train_processed.shape[1]
            }
        }

        # åŒæ—¶æ›´æ–°model_info.jsonæ–‡ä»¶
        info_path = "models/model_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump({
                'model_path': "models/lgb_model_complete.pkl",
                'model_only_path': "models/lgb_model.pkl",
                'feature_columns': list(X_train_processed.columns),
                'target_column': target_column,
                'threshold': 0.4153,  # ç¡®ä¿JSONæ–‡ä»¶ä¸­ä¹Ÿæ˜¯0.4153
                'continuous_features': ['FIB', 'IBIL'],
                'training_info': model_data['training_info']
            }, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å®Œæ•´æ¨¡å‹æ•°æ®
        model_path = "models/lgb_model_complete.pkl"
        joblib.dump(model_data, model_path)
        print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜ä¸ºPKL: {model_path}")

        # ä¿å­˜ä»…æ¨¡å‹æ–‡ä»¶ï¼ˆå…¼å®¹æ€§ï¼‰
        model_only_path = "models/lgb_model.pkl"
        joblib.dump(model, model_only_path)
        print(f"âœ… æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜: {model_only_path}")

        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        info_path = "models/model_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump({
                'model_path': model_path,
                'model_only_path': model_only_path,
                'feature_columns': feature_columns,
                'target_column': target_column,
                'threshold': eval_results['best_threshold'],
                'continuous_features': list(scalers.keys()),
                'training_info': model_data['training_info']
            }, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")

        # åŒæ—¶ä¿å­˜åŸå§‹æ ¼å¼ï¼ˆå¯é€‰ï¼‰
        save_paths = save_model_and_scalers(
            model, scalers, feature_columns, target_column,
            eval_results['best_threshold']
        )

        # 7. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
        # æ³¨æ„ï¼šä½¿ç”¨è®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„ç‰¹å¾åç§°
        plot_feature_importance(
            model, list(X_train_processed.columns),
            save_path=os.path.join("models", f"feature_importance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        )

        # 8. è¾“å‡ºæœ€ç»ˆç»“æœ
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_results['accuracy']:.4f}")
        print(f"ğŸ“Š AUC: {eval_results['auc']:.4f}")
        print(f"ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {eval_results['best_threshold']:.4f}")
        print(f"ğŸ”§ ç‰¹å¾æ•°é‡: {X_train_processed.shape[1]} ä¸ªåŸå§‹ç‰¹å¾")

        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸ºPKLæ ¼å¼")
        print(f"   ğŸ“ å®Œæ•´æ¨¡å‹: models/lgb_model_complete.pkl")
        print(f"   ğŸ“ ä»…æ¨¡å‹æ–‡ä»¶: models/lgb_model.pkl")
        print(f"   ğŸ“„ æ¨¡å‹ä¿¡æ¯: models/model_info.json")

    else:
        print("âŒ æœªæ‰¾åˆ°å®Œæ•´çš„trainã€valã€testæ•°æ®é›†")
        print(f"å¯ç”¨çš„æ•°æ®é›†: {list(dataframes.keys())}")

if __name__ == "__main__":
    main()
